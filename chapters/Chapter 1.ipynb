{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "\n",
    "This chapter introduces some common concepts about learning (such as supervised and unsupervised learning) and some simples applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "\n",
    "* Classification (labels)\n",
    "* Regression (real)\n",
    "\n",
    "Learn when we have a dataset with points and true responses variables. If we use a probabilistic approach to this kind of inference, we want to find the probability distribution of the response $y$ given the training dataset $\\mathcal{D}$  and a new point $x$ outside of it.\n",
    "\n",
    "$$p(y|x, \\mathcal{D})$$\n",
    "\n",
    "A good guess $\\hat{y}$ for $y$ is the Maximum a Posteriori estimator:\n",
    "\n",
    "$$\\underset{c}{\\mathrm{argmax}}\\ p(y = c|x, \\mathcal{D})$$\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "* Clustering\n",
    "* Dimensionality Reduction / Latent variables\n",
    "* Discovering graph structure\n",
    "* Matrix completions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric models\n",
    "\n",
    "These models have a finite (and fixed) number of parameters.\n",
    "Examples:\n",
    "* Linear regression:\n",
    "$$y(\\mathbf{x}) = \\mathbf{w}^\\intercal\\mathbf{x} + \\epsilon$$\n",
    "\n",
    "    Which can be written as\n",
    "\n",
    "$$p(y\\ |\\ x, \\theta) = \\mathcal{N}(y\\ |\\ \\mu(x), \\sigma^2) = \\mathcal{N}(y\\ |\\ w^\\intercal x, \\sigma^2)$$\n",
    "\n",
    "* Logistic regression:\n",
    "    Despite the name, this is a classification model\n",
    "    \n",
    "$$p(y\\ |\\ x, w) = \\mathrm{Ber}(y\\ |\\ \\mu(x)) = \\mathrm{Ber}(y\\ |\\ \\mathrm{sigm}(w^\\intercal x))$$\n",
    "    \n",
    "    where\n",
    "    \n",
    "$$\\displaystyle \\mathrm{sigm}(x) = \\frac{e^x}{1+e^x}$$\n",
    "### Non-parametric models\n",
    "\n",
    "These models don't have a finite number of parameters. For example the number of parameters increase with the amount of training data, as in KNN:\n",
    "\n",
    "$$p(y=c\\ |\\ x, \\mathcal{D}, K) = \\frac{1}{K} \\sum_{i \\in N_K(x, \\mathcal{D})} \\mathbb{I}(y_i = c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of dimensionality\n",
    "\n",
    "The *curse of dimensionality* refers to a series of problems that arise only when dealing with high dimensional data sets. For example, in the KNN model, if we assume the data is uniformly distributed over a $N$-dimensional cube (with high $N$), then most of the points are near its faces. Therefore, KNN loses its locality property."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
